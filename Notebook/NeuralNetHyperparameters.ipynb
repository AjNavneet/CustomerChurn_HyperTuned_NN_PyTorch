{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aed2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn==0.0\n",
    "!pip install numpy==1.21.5\n",
    "!pip install pandas==1.3.5\n",
    "!pip install scikit_learn==1.0.2\n",
    "!pip install torch==1.10.1\n",
    "!pip install matplotlib==3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7eaed9",
   "metadata": {
    "id": "7776c2d8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6880fa09",
   "metadata": {
    "id": "df8676e2"
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "df = pd.read_csv(\"../Input/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ddc24d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "77378ce7",
    "outputId": "373d002f-9293-4982-c6de-19f046bd96ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>phone_no</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>multi_screen</th>\n",
       "      <th>mail_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>100198</td>\n",
       "      <td>409-8743</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>62</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>148.35</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16.81</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>100643</td>\n",
       "      <td>340-5930</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>149</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>294.45</td>\n",
       "      <td>7.7</td>\n",
       "      <td>33.37</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>100756</td>\n",
       "      <td>372-3750</td>\n",
       "      <td>Female</td>\n",
       "      <td>65</td>\n",
       "      <td>126</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>87.30</td>\n",
       "      <td>11.9</td>\n",
       "      <td>9.89</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>101595</td>\n",
       "      <td>331-4902</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>131</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>321.30</td>\n",
       "      <td>9.5</td>\n",
       "      <td>36.41</td>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>101653</td>\n",
       "      <td>351-8398</td>\n",
       "      <td>Female</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>243.00</td>\n",
       "      <td>10.9</td>\n",
       "      <td>27.54</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  customer_id  phone_no  gender  age  no_of_days_subscribed  \\\n",
       "0  2015       100198  409-8743  Female   36                     62   \n",
       "1  2015       100643  340-5930  Female   39                    149   \n",
       "2  2015       100756  372-3750  Female   65                    126   \n",
       "3  2015       101595  331-4902  Female   24                    131   \n",
       "4  2015       101653  351-8398  Female   40                    191   \n",
       "\n",
       "  multi_screen mail_subscribed  weekly_mins_watched  minimum_daily_mins  \\\n",
       "0           no              no               148.35                12.2   \n",
       "1           no              no               294.45                 7.7   \n",
       "2           no              no                87.30                11.9   \n",
       "3           no             yes               321.30                 9.5   \n",
       "4           no              no               243.00                10.9   \n",
       "\n",
       "   maximum_daily_mins  weekly_max_night_mins  videos_watched  \\\n",
       "0               16.81                     82               1   \n",
       "1               33.37                     87               3   \n",
       "2                9.89                     91               1   \n",
       "3               36.41                    102               4   \n",
       "4               27.54                     83               7   \n",
       "\n",
       "   maximum_days_inactive  customer_support_calls  churn  \n",
       "0                    4.0                       1    0.0  \n",
       "1                    3.0                       2    0.0  \n",
       "2                    4.0                       5    1.0  \n",
       "3                    3.0                       3    0.0  \n",
       "4                    3.0                       1    0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2462d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLiXthsGRdk8",
    "outputId": "e5bf8fdf-59ba-4369-a425-f36b36c3aa21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e335f3f",
   "metadata": {
    "id": "290beada"
   },
   "outputs": [],
   "source": [
    "# Dropping columns \"customer_id\", \"phone_no\", and \"year\" from the DataFrame \"df\"\n",
    "data = df.drop([\"customer_id\", \"phone_no\", \"year\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c78bc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "e4c57431",
    "outputId": "5252ffac-163a-4dae-ae42-9eaa60389182"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>multi_screen</th>\n",
       "      <th>mail_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Female</td>\n",
       "      <td>54</td>\n",
       "      <td>75</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>182.25</td>\n",
       "      <td>11.3</td>\n",
       "      <td>20.66</td>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>127</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>273.45</td>\n",
       "      <td>9.3</td>\n",
       "      <td>30.99</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>94</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>128.85</td>\n",
       "      <td>15.6</td>\n",
       "      <td>14.60</td>\n",
       "      <td>110</td>\n",
       "      <td>16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>94</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>178.05</td>\n",
       "      <td>10.4</td>\n",
       "      <td>20.18</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Male</td>\n",
       "      <td>37</td>\n",
       "      <td>73</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>326.70</td>\n",
       "      <td>10.3</td>\n",
       "      <td>37.03</td>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  age  no_of_days_subscribed multi_screen mail_subscribed  \\\n",
       "1995  Female   54                     75           no             yes   \n",
       "1996    Male   45                    127           no              no   \n",
       "1997     NaN   53                     94           no              no   \n",
       "1998    Male   40                     94           no              no   \n",
       "1999    Male   37                     73           no              no   \n",
       "\n",
       "      weekly_mins_watched  minimum_daily_mins  maximum_daily_mins  \\\n",
       "1995               182.25                11.3               20.66   \n",
       "1996               273.45                 9.3               30.99   \n",
       "1997               128.85                15.6               14.60   \n",
       "1998               178.05                10.4               20.18   \n",
       "1999               326.70                10.3               37.03   \n",
       "\n",
       "      weekly_max_night_mins  videos_watched  maximum_days_inactive  \\\n",
       "1995                     97               5                    4.0   \n",
       "1996                    116               3                    3.0   \n",
       "1997                    110              16                    5.0   \n",
       "1998                    100               6                    NaN   \n",
       "1999                     89               6                    3.0   \n",
       "\n",
       "      customer_support_calls  churn  \n",
       "1995                       2    NaN  \n",
       "1996                       1    0.0  \n",
       "1997                       0    0.0  \n",
       "1998                       3    0.0  \n",
       "1999                       1    1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30172525",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "92e4a959",
    "outputId": "eef002b6-c397-4f1b-cc25-54fbe9ada622"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.69050</td>\n",
       "      <td>99.750000</td>\n",
       "      <td>270.178425</td>\n",
       "      <td>10.198700</td>\n",
       "      <td>30.620780</td>\n",
       "      <td>100.415500</td>\n",
       "      <td>4.482500</td>\n",
       "      <td>3.250507</td>\n",
       "      <td>1.547000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.20641</td>\n",
       "      <td>39.755386</td>\n",
       "      <td>80.551627</td>\n",
       "      <td>2.785519</td>\n",
       "      <td>9.129165</td>\n",
       "      <td>19.529454</td>\n",
       "      <td>2.487728</td>\n",
       "      <td>0.809084</td>\n",
       "      <td>1.315164</td>\n",
       "      <td>0.340021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.00000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>218.212500</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>24.735000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.00000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>269.925000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>30.590000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44.00000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>324.675000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>36.797500</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82.00000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>526.200000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.640000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  no_of_days_subscribed  weekly_mins_watched  \\\n",
       "count  2000.00000            2000.000000          2000.000000   \n",
       "mean     38.69050              99.750000           270.178425   \n",
       "std      10.20641              39.755386            80.551627   \n",
       "min      18.00000               1.000000             0.000000   \n",
       "25%      32.00000              73.000000           218.212500   \n",
       "50%      37.00000              99.000000           269.925000   \n",
       "75%      44.00000             127.000000           324.675000   \n",
       "max      82.00000             243.000000           526.200000   \n",
       "\n",
       "       minimum_daily_mins  maximum_daily_mins  weekly_max_night_mins  \\\n",
       "count         2000.000000         2000.000000            2000.000000   \n",
       "mean            10.198700           30.620780             100.415500   \n",
       "std              2.785519            9.129165              19.529454   \n",
       "min              0.000000            0.000000              42.000000   \n",
       "25%              8.400000           24.735000              87.000000   \n",
       "50%             10.200000           30.590000             101.000000   \n",
       "75%             12.000000           36.797500             114.000000   \n",
       "max             20.000000           59.640000             175.000000   \n",
       "\n",
       "       videos_watched  maximum_days_inactive  customer_support_calls  \\\n",
       "count     2000.000000            1972.000000             2000.000000   \n",
       "mean         4.482500               3.250507                1.547000   \n",
       "std          2.487728               0.809084                1.315164   \n",
       "min          0.000000               0.000000                0.000000   \n",
       "25%          3.000000               3.000000                1.000000   \n",
       "50%          4.000000               3.000000                1.000000   \n",
       "75%          6.000000               4.000000                2.000000   \n",
       "max         19.000000               6.000000                9.000000   \n",
       "\n",
       "             churn  \n",
       "count  1965.000000  \n",
       "mean      0.133333  \n",
       "std       0.340021  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727442a",
   "metadata": {
    "id": "754ec6e6"
   },
   "source": [
    "## Droping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c261496",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5bd95ab",
    "outputId": "5fcc1678-6abf-4ae2-ee45-bed10309d870",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                    24\n",
       "age                        0\n",
       "no_of_days_subscribed      0\n",
       "multi_screen               0\n",
       "mail_subscribed            0\n",
       "weekly_mins_watched        0\n",
       "minimum_daily_mins         0\n",
       "maximum_daily_mins         0\n",
       "weekly_max_night_mins      0\n",
       "videos_watched             0\n",
       "maximum_days_inactive     28\n",
       "customer_support_calls     0\n",
       "churn                     35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03eb9372",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01bc1796",
    "outputId": "6371f382-eec1-4449-d33e-f840afc142f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of data before dropping null values\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b05986",
   "metadata": {
    "id": "29c1dfbe"
   },
   "outputs": [],
   "source": [
    "# Dropping null values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb066586",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9e96a0a",
    "outputId": "1fa17ceb-4a06-4631-d062-8f248317a7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1918, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of data after dropping null values\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc721d",
   "metadata": {
    "id": "2fae0d0a"
   },
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc1aebf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47534919",
    "outputId": "f8222262-a100-4eec-b135-9d84dde4ecad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting unique categories in the \"gender\" column\n",
    "data[\"gender\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70c310c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6104bf15",
    "outputId": "457ed169-fa34-4b12-efd3-4462abfa53fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no' 'yes']\n",
      "['no' 'yes']\n"
     ]
    }
   ],
   "source": [
    "# Printing unique values in the \"multi_screen\" column\n",
    "print(data[\"multi_screen\"].unique())\n",
    "\n",
    "# Printing unique values in the \"mail_subscribed\" column\n",
    "print(data[\"mail_subscribed\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d35c58",
   "metadata": {
    "id": "b27859ff"
   },
   "outputs": [],
   "source": [
    "# Create a LabelEncoder instance for encoding categorical labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0246dac6",
   "metadata": {
    "id": "f2bce5e1"
   },
   "outputs": [],
   "source": [
    "# Encode 'gender' column\n",
    "data[\"gender\"] = le.fit_transform(data[\"gender\"])\n",
    "\n",
    "# Encode 'multi_screen' column\n",
    "data[\"multi_screen\"] = le.fit_transform(data[\"multi_screen\"])\n",
    "\n",
    "# Encode 'mail_subscribed' column\n",
    "data[\"mail_subscribed\"] = le.fit_transform(data[\"mail_subscribed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24a2821d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "24f4f5b6",
    "outputId": "edddfaff-319f-45f3-bef1-9ea49a57f865",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>multi_screen</th>\n",
       "      <th>mail_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148.35</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16.81</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>294.45</td>\n",
       "      <td>7.7</td>\n",
       "      <td>33.37</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.30</td>\n",
       "      <td>11.9</td>\n",
       "      <td>9.89</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>321.30</td>\n",
       "      <td>9.5</td>\n",
       "      <td>36.41</td>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243.00</td>\n",
       "      <td>10.9</td>\n",
       "      <td>27.54</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  age  no_of_days_subscribed  multi_screen  mail_subscribed  \\\n",
       "0       0   36                     62             0                0   \n",
       "1       0   39                    149             0                0   \n",
       "2       0   65                    126             0                0   \n",
       "3       0   24                    131             0                1   \n",
       "4       0   40                    191             0                0   \n",
       "\n",
       "   weekly_mins_watched  minimum_daily_mins  maximum_daily_mins  \\\n",
       "0               148.35                12.2               16.81   \n",
       "1               294.45                 7.7               33.37   \n",
       "2                87.30                11.9                9.89   \n",
       "3               321.30                 9.5               36.41   \n",
       "4               243.00                10.9               27.54   \n",
       "\n",
       "   weekly_max_night_mins  videos_watched  maximum_days_inactive  \\\n",
       "0                     82               1                    4.0   \n",
       "1                     87               3                    3.0   \n",
       "2                     91               1                    4.0   \n",
       "3                    102               4                    3.0   \n",
       "4                     83               7                    3.0   \n",
       "\n",
       "   customer_support_calls  churn  \n",
       "0                       1    0.0  \n",
       "1                       2    0.0  \n",
       "2                       5    1.0  \n",
       "3                       3    0.0  \n",
       "4                       1    0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "113c036d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de53a22b",
    "outputId": "d1d004d7-acf7-40ba-c8ce-6c3fb0c23c42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churn\n",
       "0.0    1665\n",
       "1.0     253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data by the \"churn\" column and count the occurrences of each unique value\n",
    "data.groupby(\"churn\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ec785",
   "metadata": {
    "id": "32940e0e"
   },
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f06297d0",
   "metadata": {
    "id": "9f45c2a6"
   },
   "outputs": [],
   "source": [
    "# Import the MinMaxScaler class from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create an instance of the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a888cd7",
   "metadata": {
    "id": "f1d9ffa7"
   },
   "outputs": [],
   "source": [
    "# Dropping specified categorical columns and keeping numerical columns\n",
    "data_num = data.drop([\"gender\", \"multi_screen\", \"mail_subscribed\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cb0bbfe",
   "metadata": {
    "id": "b1a4b97d"
   },
   "outputs": [],
   "source": [
    "# Scaling numerical columns using Min-Max scaler\n",
    "cols = data_num.columns  # Get the column names\n",
    "data_num = scaler.fit_transform(data_num)  # Apply Min-Max scaling to the numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07fd2a44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6969db66",
    "outputId": "f0bc54e3-d539-48e1-863c-f5660d55b5d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'no_of_days_subscribed',\n",
       " 'weekly_mins_watched',\n",
       " 'minimum_daily_mins',\n",
       " 'maximum_daily_mins',\n",
       " 'weekly_max_night_mins',\n",
       " 'videos_watched',\n",
       " 'maximum_days_inactive',\n",
       " 'customer_support_calls',\n",
       " 'churn']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of numerical columns\n",
    "cols = list(cols)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8099fe8",
   "metadata": {
    "id": "132b1bb7"
   },
   "outputs": [],
   "source": [
    "# assign the scaled numerical values back to the original DataFrame.\n",
    "data[cols] = data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8bc182a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "d000fc80",
    "outputId": "6ea559af-9409-4329-da2e-bdb71d749154",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>multi_screen</th>\n",
       "      <th>mail_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.252066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.281927</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.281858</td>\n",
       "      <td>0.300752</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.611570</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.559578</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165906</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.165828</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.537190</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610604</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.610496</td>\n",
       "      <td>0.451128</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.785124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.461802</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.461771</td>\n",
       "      <td>0.308271</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender       age  no_of_days_subscribed  multi_screen  mail_subscribed  \\\n",
       "0       0  0.281250               0.252066             0                0   \n",
       "1       0  0.328125               0.611570             0                0   \n",
       "2       0  0.734375               0.516529             0                0   \n",
       "3       0  0.093750               0.537190             0                1   \n",
       "4       0  0.343750               0.785124             0                0   \n",
       "\n",
       "   weekly_mins_watched  minimum_daily_mins  maximum_daily_mins  \\\n",
       "0             0.281927               0.610            0.281858   \n",
       "1             0.559578               0.385            0.559524   \n",
       "2             0.165906               0.595            0.165828   \n",
       "3             0.610604               0.475            0.610496   \n",
       "4             0.461802               0.545            0.461771   \n",
       "\n",
       "   weekly_max_night_mins  videos_watched  maximum_days_inactive  \\\n",
       "0               0.300752        0.052632               0.666667   \n",
       "1               0.338346        0.157895               0.500000   \n",
       "2               0.368421        0.052632               0.666667   \n",
       "3               0.451128        0.210526               0.500000   \n",
       "4               0.308271        0.368421               0.500000   \n",
       "\n",
       "   customer_support_calls  churn  \n",
       "0                0.111111    0.0  \n",
       "1                0.222222    0.0  \n",
       "2                0.555556    1.0  \n",
       "3                0.333333    0.0  \n",
       "4                0.111111    0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2792114b",
   "metadata": {
    "id": "eaab4d76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churn\n",
       "0.0    1665\n",
       "1.0     253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = data.drop(\"churn\", axis=1)\n",
    "# Y = data[\"churn\"].astype(int)\n",
    "\n",
    "# Prepare feature and target variables, and check class distribution.\n",
    "data.groupby(\"churn\").size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbd8245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Class Imbalance using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84faae4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dx9sEe7rTyjb",
    "outputId": "9eec5497-5b53-4cf8-a33c-87617f2e0b66",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_smote contains the resampled predictor variables, and y_smote contains the corresponding target variable.\n",
    "x_smote, y_smote = smote.fit_resample(data.iloc[:, 0:-1], data['churn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f8299b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape 1918\n",
      "Resampled dataset shape 3330\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the original dataset (length of 'data') and the shape of the resampled dataset (length of 'y_smote').\n",
    "print('Original dataset shape', len(data))\n",
    "print('Resampled dataset shape', len(y_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb783847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churn\n",
       "0.0    1665\n",
       "1.0    1665\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the class distribution after class balancing using SMOTE and display the count of each class.\n",
    "y_smote.groupby(y_smote).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ad97359",
   "metadata": {
    "id": "fc4b30f7"
   },
   "outputs": [],
   "source": [
    "# Split a dataset into train and test sets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f724e01",
   "metadata": {
    "id": "b3067ab7"
   },
   "outputs": [],
   "source": [
    "# Split the resampled data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.2, random_state=42, stratify=y_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03afd20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "# Print the counts of class 0 in the test set.\n",
    "print((y_test == 0).sum())\n",
    "\n",
    "# Print the counts of class 1 in the test set.\n",
    "print((y_test == 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbb26f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1332\n",
      "1332\n"
     ]
    }
   ],
   "source": [
    "# Print the counts of class 0 in the training set.\n",
    "print((y_train == 0).sum())\n",
    "\n",
    "# Print the counts of class 1 in the training set.\n",
    "print((y_train == 1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587213e",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6675eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the neural network:\n",
    "# - 'input_size': The number of features in the input data, which is equal to the number of columns in X_train.\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# - 'hidden_sizes': A list specifying the number of neurons in each hidden layer. In this case, there are two hidden layers with 128 and 64 neurons.\n",
    "hidden_sizes = [128, 64]\n",
    "\n",
    "# - 'output_size': The number of output neurons, which is typically the number of classes in the classification problem. In this case, it's set to 2.\n",
    "output_size = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14abee6",
   "metadata": {},
   "source": [
    "### Basic Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47fab2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Build a feed-forward neural network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),  # First linear layer: input_size to hidden_sizes[0]\n",
    "    nn.ReLU(),                               \n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),  # Second linear layer: hidden_sizes[0] to hidden_sizes[1]\n",
    "    nn.ReLU(),                               \n",
    "    nn.Linear(hidden_sizes[1], output_size),    # Third linear layer: hidden_sizes[1] to output_size\n",
    "    nn.Softmax(dim=1)                         # Softmax activation for classification\n",
    ")\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d546a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function as Negative Log Likelihood Loss (NLLLoss).\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e914627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Create an SGD optimizer for the model's parameters with a learning rate of 0.01.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "521f2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "\n",
    "# Convert X_train (predictor variables) to a PyTorch Tensor\n",
    "X_train = Tensor(X_train.values)\n",
    "\n",
    "# Convert y_train (target variable) to a PyTorch Tensor\n",
    "y_train = Tensor(np.array(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0e77828",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # Define the batch size for training\n",
    "# EPOCH = 200 \n",
    "\n",
    "# Create a PyTorch TensorDataset using X_train and y_train\n",
    "torch_dataset = Data.TensorDataset(X_train, y_train)\n",
    "\n",
    "# Create a data loader for batching and shuffling the dataset\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,  # Use the TensorDataset as the dataset\n",
    "    batch_size=BATCH_SIZE,  # Set the batch size\n",
    "    shuffle=True,           # Shuffle the data during training\n",
    "    num_workers=2,          # Use 2 workers for data loading (parallel loading)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3767f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.00789778415594731\n",
      "Training loss: -0.007905048449655195\n",
      "Training loss: -0.007917628542439\n",
      "Training loss: -0.007932962295976845\n",
      "Training loss: -0.007942577270237176\n",
      "Training loss: -0.007954382379581262\n",
      "Training loss: -0.007963758185729608\n",
      "Training loss: -0.007980194278397001\n",
      "Training loss: -0.007990321306673973\n",
      "Training loss: -0.008004433489418603\n",
      "Training loss: -0.008016235332141767\n",
      "Training loss: -0.00803038292192482\n",
      "Training loss: -0.008042726665735245\n",
      "Training loss: -0.008059851523186709\n",
      "Training loss: -0.00807207761270864\n",
      "Training loss: -0.008088584127905848\n",
      "Training loss: -0.008101366225395116\n",
      "Training loss: -0.008117792797876193\n",
      "Training loss: -0.008139217065440284\n",
      "Training loss: -0.00815721685150722\n",
      "Training loss: -0.008174789437392095\n",
      "Training loss: -0.00819857581868186\n",
      "Training loss: -0.00821892346616264\n",
      "Training loss: -0.008241440686914656\n",
      "Training loss: -0.008265849650353641\n",
      "Training loss: -0.008293946360324597\n",
      "Training loss: -0.008323486256706822\n",
      "Training loss: -0.00835746016588297\n",
      "Training loss: -0.008386917546525732\n",
      "Training loss: -0.008421928563096502\n",
      "Training loss: -0.008453218574012007\n",
      "Training loss: -0.00849340838175994\n",
      "Training loss: -0.008527141523074818\n",
      "Training loss: -0.008568319293471786\n",
      "Training loss: -0.00861268709819238\n",
      "Training loss: -0.008652752360424123\n",
      "Training loss: -0.008703184602138874\n",
      "Training loss: -0.008749537505545057\n",
      "Training loss: -0.00879638722619495\n",
      "Training loss: -0.008844650185501969\n",
      "Training loss: -0.008894019477718227\n",
      "Training loss: -0.00894890356439728\n",
      "Training loss: -0.009001921873550874\n",
      "Training loss: -0.00905060475026523\n",
      "Training loss: -0.009100108309551044\n",
      "Training loss: -0.009161010541178443\n",
      "Training loss: -0.009208951447461103\n",
      "Training loss: -0.009263534535158862\n",
      "Training loss: -0.009312425759640542\n",
      "Training loss: -0.009359456509262233\n",
      "Training loss: -0.009406070272485773\n",
      "Training loss: -0.009456298864997542\n",
      "Training loss: -0.009492636196785144\n",
      "Training loss: -0.009544929629331594\n",
      "Training loss: -0.00958754131206879\n",
      "Training loss: -0.009642038102801497\n",
      "Training loss: -0.00967519558048821\n",
      "Training loss: -0.009716469201597723\n",
      "Training loss: -0.009744224158135263\n",
      "Training loss: -0.009781500218329844\n",
      "Training loss: -0.009815675740664428\n",
      "Training loss: -0.009858143401217533\n",
      "Training loss: -0.009888492233760364\n",
      "Training loss: -0.009924925707124017\n",
      "Training loss: -0.009943772006679225\n",
      "Training loss: -0.009991308008586322\n",
      "Training loss: -0.010027800527241853\n",
      "Training loss: -0.010047747387184395\n",
      "Training loss: -0.010077421215024439\n",
      "Training loss: -0.010106821385052827\n",
      "Training loss: -0.010141082980611303\n",
      "Training loss: -0.010161809600867308\n",
      "Training loss: -0.010194864314239662\n",
      "Training loss: -0.010231744195963885\n",
      "Training loss: -0.010257178240710192\n",
      "Training loss: -0.010262199171312578\n",
      "Training loss: -0.010306669359987563\n",
      "Training loss: -0.010323946741787163\n",
      "Training loss: -0.010370144861059504\n",
      "Training loss: -0.0103788302467392\n",
      "Training loss: -0.010409444503419034\n",
      "Training loss: -0.010405588995765996\n",
      "Training loss: -0.010446378955611953\n",
      "Training loss: -0.010472874696906264\n",
      "Training loss: -0.010481148130363889\n",
      "Training loss: -0.010516740471214146\n",
      "Training loss: -0.010536689098711844\n",
      "Training loss: -0.01055429014447215\n",
      "Training loss: -0.0105799231398571\n",
      "Training loss: -0.010600612798073629\n",
      "Training loss: -0.010619751862935475\n",
      "Training loss: -0.010636618388844683\n",
      "Training loss: -0.010657520847277599\n",
      "Training loss: -0.010670630602507261\n",
      "Training loss: -0.010685367761431513\n",
      "Training loss: -0.010727371942173611\n",
      "Training loss: -0.010745153591797516\n",
      "Training loss: -0.01075037148174223\n",
      "Training loss: -0.010762370630606517\n",
      "Training loss: -0.01077961143072661\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "epochs = 100  # Number of training epochs\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "\n",
    "        b_x = Variable(batch_x)  # Convert batch_x to a PyTorch Variable\n",
    "        b_y = Variable(batch_y.type(torch.LongTensor))  # Convert batch_y to a PyTorch Variable with the correct data type\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "        output = model(b_x)  # Forward pass through the model\n",
    "        loss = criterion(output, b_y)  # Calculate the loss\n",
    "        loss.backward()  # Backpropagation to compute gradients\n",
    "        optimizer.step()  # Update model parameters using the optimizer\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss / len(X_train)}\")  # Print the average loss for the current epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f08eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test data 'X_test' to PyTorch Tensor\n",
    "X_test_tensor = Tensor(X_test.values)\n",
    "\n",
    "# Convert the test target variable 'y_test' to PyTorch Tensor\n",
    "y_test = Tensor(np.array(y_test))\n",
    "\n",
    "# Perform inference (forward pass) on the test data using the trained model 'model'\n",
    "z = model(X_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4a27483",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8351d8a6",
    "outputId": "3d57a4f8-4322-4d2e-aba8-7d27d39d0701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Test Data  70.12012012012012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the model's predictions 'z' to a list of predicted class labels\n",
    "yhat = list(z.argmax(1))\n",
    "\n",
    "# Convert the test target variable 'y_test' to a list\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Calculate the accuracy score by comparing the predicted labels with the true labels and print the result\n",
    "print(\"Accuracy Score of Test Data:\", accuracy_score(y_test, yhat) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b956a",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff29ceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (1): Dropout(p=0.2, inplace=False)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): Dropout(p=0.2, inplace=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (7): Dropout(p=0.1, inplace=False)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (10): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the architecture of a feed-forward neural network with dropout layers.\n",
    "hidden_sizes = [128, 64, 32, 16]  # Define the number of neurons in each hidden layer.\n",
    "\n",
    "model_dropout = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),  # First linear layer\n",
    "    nn.Dropout(0.2),  # Dropout layer with a 20% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),  # Second linear layer\n",
    "    nn.Dropout(0.2),  # Dropout layer with a 20% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[2]),  # Third linear layer\n",
    "    nn.Dropout(0.1),  # Dropout layer with a 10% probability of dropout\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_sizes[2], output_size),  # Fourth linear layer\n",
    "    nn.Softmax(dim=1)  # Softmax activation for classification\n",
    ")\n",
    "\n",
    "# Print the architecture of the model with dropout layers.\n",
    "print(model_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e80f714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function as Negative Log Likelihood Loss (NLLLoss).\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Create an SGD optimizer for the model_dropout's parameters with a learning rate of 0.01.\n",
    "optimizer = optim.SGD(model_dropout.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d22ef1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.007888370638226604\n",
      "Training loss: -0.007894002728992038\n",
      "Training loss: -0.007897055990345127\n",
      "Training loss: -0.007894492431266888\n",
      "Training loss: -0.007900295169414344\n",
      "Training loss: -0.007897352134143268\n",
      "Training loss: -0.00790988518534838\n",
      "Training loss: -0.007908739809606885\n",
      "Training loss: -0.007911512052064185\n",
      "Training loss: -0.007911378411469874\n",
      "Training loss: -0.007909666836351246\n",
      "Training loss: -0.007914458074279734\n",
      "Training loss: -0.007920038089290395\n",
      "Training loss: -0.007917347131996183\n",
      "Training loss: -0.007925467123766919\n",
      "Training loss: -0.007936260005733272\n",
      "Training loss: -0.00793001128433345\n",
      "Training loss: -0.007932183766239756\n",
      "Training loss: -0.007931865561563332\n",
      "Training loss: -0.007945480390085472\n",
      "Training loss: -0.00794787975671413\n",
      "Training loss: -0.007954389740665396\n",
      "Training loss: -0.007954380276414368\n",
      "Training loss: -0.007964967327701437\n",
      "Training loss: -0.007968773444493612\n",
      "Training loss: -0.007963658688036172\n",
      "Training loss: -0.007969736437629294\n",
      "Training loss: -0.007973189468498345\n",
      "Training loss: -0.007978837500821363\n",
      "Training loss: -0.007995456613756873\n",
      "Training loss: -0.007991553583481649\n",
      "Training loss: -0.008002390430585758\n",
      "Training loss: -0.008017508743761538\n",
      "Training loss: -0.008014253958746477\n",
      "Training loss: -0.008026946045167453\n",
      "Training loss: -0.008037968910671212\n",
      "Training loss: -0.008042463590880414\n",
      "Training loss: -0.008046575819139366\n",
      "Training loss: -0.008066584409267694\n",
      "Training loss: -0.008057935370339287\n",
      "Training loss: -0.008076539906385067\n",
      "Training loss: -0.008080358787610367\n",
      "Training loss: -0.008115611735496435\n",
      "Training loss: -0.008119078537633826\n",
      "Training loss: -0.0081037000806124\n",
      "Training loss: -0.008154367731587665\n",
      "Training loss: -0.008133870759257325\n",
      "Training loss: -0.00816923179649734\n",
      "Training loss: -0.008161738347094338\n",
      "Training loss: -0.008195666400519936\n",
      "Training loss: -0.00822270227534635\n",
      "Training loss: -0.008228207985918085\n",
      "Training loss: -0.00825601942591123\n",
      "Training loss: -0.00825270304495508\n",
      "Training loss: -0.008283770399856137\n",
      "Training loss: -0.00832418137573981\n",
      "Training loss: -0.00834644905797712\n",
      "Training loss: -0.008350562080517187\n",
      "Training loss: -0.008385072778295111\n",
      "Training loss: -0.008420332706905342\n",
      "Training loss: -0.008429034884359027\n",
      "Training loss: -0.008417273147238625\n",
      "Training loss: -0.008478185503153471\n",
      "Training loss: -0.008492277951927873\n",
      "Training loss: -0.008522230561252113\n",
      "Training loss: -0.008533520159778651\n",
      "Training loss: -0.008606643961356566\n",
      "Training loss: -0.00861344146388429\n",
      "Training loss: -0.008645615453745151\n",
      "Training loss: -0.008649907782450094\n",
      "Training loss: -0.008735453771488802\n",
      "Training loss: -0.008756648200947244\n",
      "Training loss: -0.008816959800662938\n",
      "Training loss: -0.008849326398100581\n",
      "Training loss: -0.008854863477182819\n",
      "Training loss: -0.008906090469868691\n",
      "Training loss: -0.008960307807893725\n",
      "Training loss: -0.00901469818733118\n",
      "Training loss: -0.009094588346667477\n",
      "Training loss: -0.009067337299968387\n",
      "Training loss: -0.00912222351218845\n",
      "Training loss: -0.009164740619537709\n",
      "Training loss: -0.009218000882380718\n",
      "Training loss: -0.00921421254182363\n",
      "Training loss: -0.00928864274565522\n",
      "Training loss: -0.009331779727885674\n",
      "Training loss: -0.009449241062005361\n",
      "Training loss: -0.009424910083547369\n",
      "Training loss: -0.009484890591095876\n",
      "Training loss: -0.009516529738903046\n",
      "Training loss: -0.009561376170711117\n",
      "Training loss: -0.009612804552814266\n",
      "Training loss: -0.00966491228019869\n",
      "Training loss: -0.009699256682360137\n",
      "Training loss: -0.009733692393288598\n",
      "Training loss: -0.00974066530888503\n",
      "Training loss: -0.009857006303898923\n",
      "Training loss: -0.009912581996874767\n",
      "Training loss: -0.009966729527658171\n",
      "Training loss: -0.009937351329011601\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "epochs = 100  # Number of training epochs\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "\n",
    "        b_x = Variable(batch_x)  # Convert batch_x to a PyTorch Variable\n",
    "        b_y = Variable(batch_y.type(torch.LongTensor))  # Convert batch_y to a PyTorch Variable with the correct data type\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "        output = model_dropout(b_x)  # Forward pass through the model_dropout\n",
    "        loss = criterion(output, b_y)  # Calculate the loss\n",
    "        loss.backward()  # Backpropagation to compute gradients\n",
    "        optimizer.step()  # Update model_dropout's parameters using the optimizer\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss / len(X_train)}\")  # Print the average loss for the current epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15839cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Test Data  66.96696696696696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the test data 'X_test' to PyTorch Tensor\n",
    "X_test_tensor = Tensor(X_test.values)\n",
    "\n",
    "# Convert the test target variable 'y_test' to a PyTorch Tensor\n",
    "y_test = Tensor(np.array(y_test))\n",
    "\n",
    "# Perform inference (forward pass) on the test data using the model with dropout layers ('model_dropout')\n",
    "z = model_dropout(X_test_tensor)\n",
    "\n",
    "# Convert the model's predictions 'z' to a list of predicted class labels\n",
    "yhat = list(z.argmax(1))\n",
    "\n",
    "# Convert the test target variable 'y_test' to a list\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Calculate the accuracy score by comparing the predicted labels with the true labels and print the result\n",
    "print(\"Accuracy Score of Test Data:\", accuracy_score(y_test, yhat) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa6cce",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e9951e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (1): Dropout(p=0.2, inplace=False)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): Dropout(p=0.2, inplace=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (7): Dropout(p=0.1, inplace=False)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (10): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the architecture of a feed-forward neural network with dropout layers.\n",
    "hidden_sizes = [128, 64]  # Define the number of neurons in each hidden layer.\n",
    "\n",
    "model_reg = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),  # First linear layer\n",
    "    nn.Dropout(0.2),  # Dropout layer with a 20% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),  # Second linear layer\n",
    "    nn.Dropout(0.2),  # Dropout layer with a 20% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[1]),  # Third linear layer (hidden_sizes[1] repeated)\n",
    "    nn.Dropout(0.1),  # Dropout layer with a 10% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[1], output_size),  # Fourth linear layer\n",
    "    nn.Softmax(dim=1)  # Softmax activation for classification\n",
    ")\n",
    "\n",
    "# Print the architecture of the model with dropout layers.\n",
    "print(model_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a356bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function as Negative Log Likelihood Loss (NLLLoss).\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Import the optimizer module and create an Adam optimizer for the 'model_reg' parameters.\n",
    "# Set the learning rate to 1e-4 and add L2 regularization by specifying the 'weight_decay' parameter.\n",
    "optimizer = optim.Adam(model_reg.parameters(), lr=1e-4, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "824c978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.007889933548532091\n",
      "Training loss: -0.00792435002890793\n",
      "Training loss: -0.00796551549354115\n",
      "Training loss: -0.008046041883237369\n",
      "Training loss: -0.00816335097268537\n",
      "Training loss: -0.008385614276648284\n",
      "Training loss: -0.00871310897686102\n",
      "Training loss: -0.009047251988996615\n",
      "Training loss: -0.009459709015873459\n",
      "Training loss: -0.009751390788827214\n",
      "Training loss: -0.01008074426346713\n",
      "Training loss: -0.010240384855785885\n",
      "Training loss: -0.010439680369050653\n",
      "Training loss: -0.010566812444914569\n",
      "Training loss: -0.010715384092237856\n",
      "Training loss: -0.01082327841101466\n",
      "Training loss: -0.010916096066032444\n",
      "Training loss: -0.011006011126993655\n",
      "Training loss: -0.011152815174412084\n",
      "Training loss: -0.011240321997407678\n",
      "Training loss: -0.011230370952739372\n",
      "Training loss: -0.011226870455183424\n",
      "Training loss: -0.01128714668142187\n",
      "Training loss: -0.011344281559412903\n",
      "Training loss: -0.011517359643011121\n",
      "Training loss: -0.011538142869780372\n",
      "Training loss: -0.011599198296979384\n",
      "Training loss: -0.011695966773383968\n",
      "Training loss: -0.011738097591622098\n",
      "Training loss: -0.011797867066509373\n",
      "Training loss: -0.011746877709308544\n",
      "Training loss: -0.01170966490073963\n",
      "Training loss: -0.011844859735385791\n",
      "Training loss: -0.011867228213015262\n",
      "Training loss: -0.011865874042024126\n",
      "Training loss: -0.011879479003561151\n",
      "Training loss: -0.011910437776877716\n",
      "Training loss: -0.01204483302774372\n",
      "Training loss: -0.011920731458757017\n",
      "Training loss: -0.012033439411952332\n",
      "Training loss: -0.012024187894017846\n",
      "Training loss: -0.012049662749151568\n",
      "Training loss: -0.012078882069201083\n",
      "Training loss: -0.0121021279030376\n",
      "Training loss: -0.012149138180343239\n",
      "Training loss: -0.012151360713146828\n",
      "Training loss: -0.012163960159540892\n",
      "Training loss: -0.012185353752192075\n",
      "Training loss: -0.012142687141000329\n",
      "Training loss: -0.012257893208984856\n",
      "Training loss: -0.012206628202676057\n",
      "Training loss: -0.012229117343912611\n",
      "Training loss: -0.012230176310818474\n",
      "Training loss: -0.012203402123651706\n",
      "Training loss: -0.012298804660280188\n",
      "Training loss: -0.012293114497496918\n",
      "Training loss: -0.012285013613221166\n",
      "Training loss: -0.012269031715106678\n",
      "Training loss: -0.012351168906903482\n",
      "Training loss: -0.012360171333805577\n",
      "Training loss: -0.012377066476209028\n",
      "Training loss: -0.012374310387863411\n",
      "Training loss: -0.012319495862310714\n",
      "Training loss: -0.012407252537058638\n",
      "Training loss: -0.01233292912488227\n",
      "Training loss: -0.012383566559614005\n",
      "Training loss: -0.012455126231497115\n",
      "Training loss: -0.012463515607623366\n",
      "Training loss: -0.01245886555036625\n",
      "Training loss: -0.012500132548737454\n",
      "Training loss: -0.012479974342896058\n",
      "Training loss: -0.01248294799714475\n",
      "Training loss: -0.012411601237348607\n",
      "Training loss: -0.01243715359641983\n",
      "Training loss: -0.01248030905967002\n",
      "Training loss: -0.012542681874813619\n",
      "Training loss: -0.012568720535294071\n",
      "Training loss: -0.012545048049441329\n",
      "Training loss: -0.012465417944454216\n",
      "Training loss: -0.012478343672580548\n",
      "Training loss: -0.012544329034852551\n",
      "Training loss: -0.012600977521937888\n",
      "Training loss: -0.012536038664189187\n",
      "Training loss: -0.012519122713857942\n",
      "Training loss: -0.012578657887003443\n",
      "Training loss: -0.012655728236512022\n",
      "Training loss: -0.012630759685247153\n",
      "Training loss: -0.012610491488251958\n",
      "Training loss: -0.01264519005029409\n",
      "Training loss: -0.01260439988908109\n",
      "Training loss: -0.012650146968550869\n",
      "Training loss: -0.012583574218613966\n",
      "Training loss: -0.012523562790037275\n",
      "Training loss: -0.012662344307333857\n",
      "Training loss: -0.012679422872739512\n",
      "Training loss: -0.012658352116206745\n",
      "Training loss: -0.012661570409038762\n",
      "Training loss: -0.012610697620981806\n",
      "Training loss: -0.012683203516600726\n",
      "Training loss: -0.012708814988086174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "epochs = 100  # Number of training epochs\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "\n",
    "        b_x = Variable(batch_x)  # Convert batch_x to a PyTorch Variable\n",
    "        b_y = Variable(batch_y.type(torch.LongTensor))  # Convert batch_y to a PyTorch Variable with the correct data type\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "        output = model_reg(b_x)  # Forward pass through the model_reg\n",
    "        loss = criterion(output, b_y)  # Calculate the loss\n",
    "        loss.backward()  # Backpropagation to compute gradients\n",
    "        optimizer.step()  # Update model_reg's parameters using the optimizer\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss / len(X_train)}\")  # Print the average loss for the current epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fc12823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Test Data  77.47747747747748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the test data 'X_test' to PyTorch Tensor\n",
    "X_test_tensor = Tensor(X_test.values)\n",
    "\n",
    "# Convert the test target variable 'y_test' to a PyTorch Tensor\n",
    "y_test = Tensor(np.array(y_test))\n",
    "\n",
    "# Perform inference (forward pass) on the test data using the model with dropout layers and L2 regularization ('model_reg')\n",
    "z = model_reg(X_test_tensor)\n",
    "\n",
    "# Convert the model's predictions 'z' to a list of predicted class labels\n",
    "yhat = list(z.argmax(1))\n",
    "\n",
    "# Convert the test target variable 'y_test' to a list\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Calculate the accuracy score by comparing the predicted labels with the true labels and print the result\n",
    "print(\"Accuracy Score of Test Data:\", accuracy_score(y_test, yhat) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a14a50",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bb86e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (1): Dropout(p=0.2, inplace=False)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): Dropout(p=0.2, inplace=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (7): Dropout(p=0.1, inplace=False)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (10): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the architecture of a feed-forward neural network with dropout layers.\n",
    "\n",
    "hidden_sizes = [128, 64]  # Define the number of neurons in each hidden layer.\n",
    "\n",
    "model_early_stp = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),  # First linear layer\n",
    "    nn.Dropout(0.2),  # Dropout layer with a 20% probability of dropout\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),  # Second linear layer\n",
    "    nn.Dropout(0.2),  # Dropout layer with a 20% probability of dropout\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[1]),  # Third linear layer (hidden_sizes[1] repeated)\n",
    "    nn.Dropout(0.1),  # Dropout layer with a 10% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[1], output_size),  # Fourth linear layer\n",
    "    nn.Softmax(dim=1)  # Softmax activation for classification\n",
    ")\n",
    "\n",
    "# Print the architecture of the model with dropout layers.\n",
    "print(model_early_stp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab2ef9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function as Negative Log Likelihood Loss (NLLLoss).\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Import the optimizer module and create an Adam optimizer for the 'model_early_stp' parameters.\n",
    "# Set the learning rate to 1e-4 and add L2 regularization by specifying the 'weight_decay' parameter.\n",
    "optimizer = optim.Adam(model_early_stp.parameters(), lr=1e-4, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "401952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100  # Number of training epochs\n",
    "epochs_no_improve = 0  # Counter for epochs with no improvement\n",
    "early_stop = False  # A flag to indicate whether early stopping criteria have been met\n",
    "min_loss = np.Inf  # Initialize a variable to store the minimum loss observed\n",
    "iter = 0  # Iteration counter (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5400acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.007904763817339688\n",
      "Training loss: -0.007935701145066155\n",
      "Training loss: -0.007988138308933189\n",
      "Training loss: -0.008100458496325725\n",
      "Training loss: -0.008247762448615857\n",
      "Training loss: -0.00849939634402593\n",
      "Training loss: -0.008786714135168551\n",
      "Training loss: -0.009152155291210782\n",
      "Training loss: -0.009548420371773007\n",
      "Training loss: -0.009894377767323732\n",
      "Training loss: -0.01019603262344996\n",
      "Training loss: -0.01040448259573441\n",
      "Training loss: -0.01058105247335749\n",
      "Training loss: -0.010690960171702388\n",
      "Training loss: -0.010919621756842902\n",
      "Training loss: -0.010918568092602509\n",
      "Training loss: -0.010990522399141982\n",
      "Training loss: -0.011169575132407225\n",
      "Training loss: -0.011149736182825701\n",
      "Training loss: -0.011277639136479065\n",
      "Training loss: -0.011329607741610782\n",
      "Training loss: -0.01143415263286224\n",
      "Training loss: -0.011387877882242919\n",
      "Training loss: -0.01151456985924695\n",
      "Training loss: -0.011481568083032832\n",
      "Training loss: -0.011505221662757633\n",
      "Training loss: -0.011605710440032833\n",
      "Training loss: -0.011589029171803334\n",
      "Training loss: -0.011623137840279588\n",
      "Training loss: -0.01161299466907799\n",
      "Training loss: -0.011657445235653324\n",
      "Training loss: -0.011635905405780574\n",
      "Training loss: -0.011723200122157374\n",
      "Training loss: -0.011717973148321605\n",
      "Training loss: -0.011685103715003075\n",
      "Training loss: -0.01185514469135989\n",
      "Training loss: -0.01182853092660417\n",
      "Training loss: -0.011883162924119303\n",
      "Training loss: -0.011871542927023169\n",
      "Training loss: -0.011884320918862169\n",
      "Training loss: -0.01188723702688475\n",
      "Training loss: -0.011909291976028018\n",
      "Training loss: -0.012021820824425499\n",
      "Training loss: -0.011950032175840201\n",
      "Training loss: -0.012123169528471457\n",
      "Training loss: -0.012030537086385148\n",
      "Training loss: -0.012171325919864414\n",
      "Training loss: -0.012150368040746398\n",
      "Training loss: -0.012145795107066811\n",
      "Training loss: -0.012230691228721951\n",
      "Training loss: -0.01217196349267129\n",
      "Training loss: -0.012206791757463335\n",
      "Training loss: -0.012294972667822967\n",
      "Training loss: -0.012262226381638387\n",
      "Training loss: -0.012203016617634633\n",
      "Training loss: -0.01231678512629804\n",
      "Training loss: -0.012310859140333112\n",
      "Training loss: -0.012348514508914662\n",
      "Training loss: -0.012322202593356639\n",
      "Training loss: -0.012324550085597567\n",
      "Training loss: -0.012370994096403723\n",
      "Training loss: -0.012312634772545582\n",
      "Training loss: -0.012343488678380891\n",
      "Training loss: -0.012398452014178486\n",
      "Training loss: -0.012415898029689674\n",
      "Training loss: -0.012416031043808739\n",
      "Training loss: -0.012390208741029104\n",
      "Training loss: -0.012448740725939697\n",
      "Training loss: -0.01253603269030024\n",
      "Training loss: -0.012444748691431395\n",
      "Training loss: -0.01244227469593913\n",
      "Training loss: -0.012566799806939947\n",
      "Training loss: -0.01248097728501569\n",
      "Training loss: -0.012472810240479203\n",
      "Training loss: -0.012512025331054721\n",
      "Training loss: -0.012440935045749217\n",
      "Training loss: -0.012578554496213838\n",
      "Training loss: -0.012512071018999404\n",
      "Training loss: -0.012595734528235128\n",
      "Training loss: -0.012579307899818764\n",
      "Training loss: -0.012537952009085062\n",
      "Training loss: -0.012627209561902124\n",
      "Training loss: -0.012582378970968115\n",
      "Training loss: -0.012604991729195053\n",
      "Training loss: -0.01253988168708555\n",
      "Training loss: -0.012522866910284347\n",
      "Training loss: -0.0126046881124422\n",
      "Training loss: -0.012669688208146138\n",
      "Training loss: -0.012711407834882135\n",
      "Training loss: -0.012616088731331868\n",
      "Training loss: -0.012747238683807957\n",
      "Training loss: -0.012638196282021634\n",
      "Training loss: -0.012723720176620884\n",
      "Training loss: -0.012726229567964514\n",
      "Training loss: -0.012715945037098619\n",
      "Training loss: -0.012752598671762793\n",
      "Training loss: -0.01261833547292887\n",
      "Training loss: -0.012654139920397921\n",
      "Training loss: -0.012753466720337624\n",
      "Training loss: -0.012680617180672494\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    if early_stop:\n",
    "        print(\"Stopped\")\n",
    "        break\n",
    "    else:\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            b_x = Variable(batch_x)\n",
    "            b_y = Variable(batch_y.type(torch.LongTensor))\n",
    "\n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model_early_stp(b_x)\n",
    "            loss = criterion(output, b_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Check for early stopping conditions\n",
    "            if abs(running_loss) < abs(min_loss):\n",
    "                epochs_no_improve = 0\n",
    "                min_loss = running_loss\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            iter += 1\n",
    "\n",
    "            # Check for early stopping based on a condition (e.g., no improvement for 'epochs' epochs)\n",
    "            if e > 5 and epochs_no_improve == epochs:\n",
    "                print('Early stopping!')\n",
    "                early_stop = True\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(f\"Training loss: {running_loss/len(X_train)}\")  # Print the average loss for the current epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "295fa33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Test Data  79.72972972972973\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the test data 'X_test' to PyTorch Tensor\n",
    "X_test_tensor = Tensor(X_test.values)\n",
    "\n",
    "# Convert the test target variable 'y_test' to a PyTorch Tensor\n",
    "y_test = Tensor(np.array(y_test))\n",
    "\n",
    "# Perform inference (forward pass) on the test data using the model with dropout layers and early stopping ('model_early_stp')\n",
    "z = model_early_stp(X_test_tensor)\n",
    "\n",
    "# Convert the model's predictions 'z' to a list of predicted class labels\n",
    "yhat = list(z.argmax(1))\n",
    "\n",
    "# Convert the test target variable 'y_test' to a list\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Calculate the accuracy score by comparing the predicted labels with the true labels and print the result\n",
    "print(\"Accuracy Score of Test Data:\", accuracy_score(y_test, yhat) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904635b1",
   "metadata": {},
   "source": [
    "### Checkpoint (Loading and saving model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a5cd673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (1): Dropout(p=0.2, inplace=False)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): Dropout(p=0.2, inplace=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (7): Dropout(p=0.1, inplace=False)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (10): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the architecture of a feed-forward neural network with dropout layers.\n",
    "hidden_sizes = [128, 64]  # Define the number of neurons in each hidden layer.\n",
    "\n",
    "model_chk = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),  # First linear layer\n",
    "    nn.Dropout(0.2),  # Dropout layer with a 20% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),  # Second linear layer\n",
    "    nn.Dropout(0.2),  # Dropout layer with a 20% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[1], hidden_sizes[1]),  # Third linear layer (hidden_sizes[1] repeated)\n",
    "    nn.Dropout(0.1),  # Dropout layer with a 10% probability of dropout\n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(hidden_sizes[1], output_size),  # Fourth linear layer\n",
    "    nn.Softmax(dim=1)  # Softmax activation for classification\n",
    ")\n",
    "\n",
    "# Print the architecture of the model with dropout layers.\n",
    "print(model_chk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74b5c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function as Negative Log Likelihood Loss (NLLLoss).\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Import the optimizer module and create an Adam optimizer for the 'model_chk' parameters.\n",
    "# Set the learning rate to 1e-4 and add L2 regularization by specifying the 'weight_decay' parameter.\n",
    "optimizer = optim.Adam(model_chk.parameters(), lr=1e-4, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "458b5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3  # Number of training epochs\n",
    "path = \"../model\"  # File path for saving or loading the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17e1bd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.007902685004669625\n",
      "Training loss: -0.007933632423122366\n",
      "Training loss: -0.007970469637407555\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y.type(torch.LongTensor))\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model_chk(b_x)\n",
    "        loss = criterion(output, b_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Save a model checkpoint with current state\n",
    "        torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model_chk.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "        }, path + \"model_\" + str(e) + \".pt\")\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\")  # Print the average loss for the current epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b009f1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=12, out_features=128, bias=True)\n",
       "  (1): Dropout(p=0.2, inplace=False)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (4): Dropout(p=0.2, inplace=False)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (7): Dropout(p=0.1, inplace=False)\n",
       "  (8): ReLU()\n",
       "  (9): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (10): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load = model_chk  # Create a new instance of the model (assuming 'model_chk' is the same architecture as the pre-trained model)\n",
    "optimizer = optim.Adam(model_reg.parameters(), lr=1e-4, weight_decay=1e-5)  # Create a new optimizer for the model\n",
    "\n",
    "# Load a pre-trained model checkpoint (epoch 2 in this case)\n",
    "checkpoint = torch.load(path + \"model_2.pt\")\n",
    "\n",
    "# Load the state dictionary of the pre-trained model and optimizer\n",
    "model_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "epoch = checkpoint['epoch']  # Retrieve the epoch number from the checkpoint\n",
    "loss = checkpoint['loss']  # Retrieve the loss from the checkpoint\n",
    "\n",
    "model_load.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bac6d81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Test Data  65.16516516516516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the test data 'X_test' to PyTorch Tensor\n",
    "X_test_tensor = Tensor(X_test.values)\n",
    "\n",
    "# Convert the test target variable 'y_test' to a PyTorch Tensor\n",
    "y_test = Tensor(np.array(y_test))\n",
    "\n",
    "# Perform inference (forward pass) on the test data using the pre-trained model ('model_load')\n",
    "z = model_load(X_test_tensor)\n",
    "\n",
    "# Convert the model's predictions 'z' to a list of predicted class labels\n",
    "yhat = list(z.argmax(1))\n",
    "\n",
    "# Convert the test target variable 'y_test' to a list\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Calculate the accuracy score by comparing the predicted labels with the true labels and print the result\n",
    "print(\"Accuracy Score of Test Data:\", accuracy_score(y_test, yhat) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7aa03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba3b67",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
